{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "############################################## Stemming and Lemmatization ######################################################"
      ],
      "metadata": {
        "id": "HF0t7LYj2lWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Q39fsGdr4Pa",
        "outputId": "cdce6e50-db47-41c6-c3c9-485c0b6f2894"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "# Import necessary libraries from nltk for natural language processing.\n",
        "# nltk.stem contains modules for stemming, and nltk.tokenize contains modules for tokenization.\n",
        "# PorterStemmer is a specific stemming algorithm, and word_tokenize is a function for splitting text into words.\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the 'punkt' tokenizer models. This is required for word_tokenize to work.\n",
        "# 'punkt' is a pre-trained model that helps in splitting text into sentences and words.\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the PorterStemmer.\n",
        "# This creates an instance of the stemmer object that will be used to perform stemming on words.\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "nDVIGgd5uxTm"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a sample sentence to demonstrate tokenization and stemming.\n",
        "# This sentence will be used as input for the subsequent NLP operations.\n",
        "sentence= \"The runner were running in a race and they ran very fast\""
      ],
      "metadata": {
        "id": "42-LtsuUu61W"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrate stemming on a single word.\n",
        "# The stemmer.stem() method reduces the word \"history\" to its root form \"histori\".\n",
        "# This shows how the PorterStemmer algorithm works on individual words.\n",
        "stemmer.stem(\"history\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "7zM-8FAKvHYu",
        "outputId": "8d94efb2-c104-4a75-84cc-879d29331065"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'histori'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the sample sentence into individual words.\n",
        "# word_tokenize from nltk.tokenize splits the sentence string into a list of words.\n",
        "# This is a necessary step to process text word by word.\n",
        "tokens= word_tokenize(sentence)\n",
        "# Display the list of tokens.\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_lBiBcJvKmc",
        "outputId": "950e8955-9c77-41cf-e87a-44d53a66f4b9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'runner',\n",
              " 'were',\n",
              " 'running',\n",
              " 'in',\n",
              " 'a',\n",
              " 'race',\n",
              " 'and',\n",
              " 'they',\n",
              " 'ran',\n",
              " 'very',\n",
              " 'fast']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply stemming to each word in the tokenized list.\n",
        "# This uses a list comprehension to iterate through the 'tokens' and apply the stemmer.stem() method.\n",
        "stemmer_word = [stemmer.stem(word) for word in tokens]\n",
        "# Print the list of stemmed words.\n",
        "print(stemmer_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVbhCs3tvdyv",
        "outputId": "c13f6405-a132-4b0d-9c70-8e02aad092d0"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'runner', 'were', 'run', 'in', 'a', 'race', 'and', 'they', 'ran', 'veri', 'fast']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for lemmatization.\n",
        "# nltk.stem provides WordNetLemmatizer, and nltk.corpus provides WordNet data.\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "# Download the 'wordnet' corpus, which is needed for lemmatization.\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBkoYgb9wH1Z",
        "outputId": "99b19247-43e5-4900-b726-4786908aab33"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the WordNetLemmatizer.\n",
        "# This creates an instance of the lemmatizer object.\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "UfOa5T8owd9w"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply lemmatization to each word in the tokens list, specifying the part of speech as verb (\"v\").\n",
        "# This uses a list comprehension to iterate and apply lemmatizer.lemmatize() with the specified pos.\n",
        "lemmatizer_word = [lemmatizer.lemmatize(word,pos=\"v\") for word in tokens]"
      ],
      "metadata": {
        "id": "ZFTn3MLGwjgo"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the original tokens and the lemmatized words for comparison.\n",
        "print(tokens)\n",
        "print(lemmatizer_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyGwf2ZqwzGG",
        "outputId": "4cb3670d-0070-4f17-be2f-ea03c854f324"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'runner', 'were', 'running', 'in', 'a', 'race', 'and', 'they', 'ran', 'very', 'fast']\n",
            "['The', 'runner', 'be', 'run', 'in', 'a', 'race', 'and', 'they', 'run', 'very', 'fast']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a sample text containing HTML tags.\n",
        "text = \"<html><body><p> Movie 1</p><p> Actor - Aamir Khan</p><p> Click here to <a href='http://google.com'>download</a></p></body></html>\"\n",
        "\n",
        "# Define a function to remove HTML tags using regular expressions.\n",
        "import re\n",
        "def remove_html_tags(text):\n",
        "    # Define a regex pattern to match HTML tags.\n",
        "    pattern = re.compile('<.*?>')\n",
        "    # Use the sub() method to replace the matched tags with an empty string.\n",
        "    return pattern.sub(r'',text)\n",
        "\n",
        "# Print the text after removing HTML tags.\n",
        "print(remove_html_tags(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcQd1pFAxFqG",
        "outputId": "e4a996e4-d05b-42ee-9e7b-dc6bd0e03a41"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Movie 1 Actor - Aamir Khan Click here to download\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a sample text containing a URL to demonstrate URL removal.\n",
        "text = \"<html><body><p> Movie 1</p><p> Actor - Aamir Khan</p><p> Click here to <a href='http://google.com'>download</a></p></body></html>\"\n",
        "\n",
        "# Define a function to remove URLs from text using regular expressions.\n",
        "import re # Import the regular expression module.\n",
        "def remove_url(text):\n",
        "    # Define a regex pattern to match URLs (http, https, or www).\n",
        "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    # Use sub() to replace matched URLs with an empty string.\n",
        "    return pattern.sub(r'',text)\n",
        "\n",
        "# Print the text after removing the URL.\n",
        "print(remove_url(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_20-AZUx3nJ",
        "outputId": "bc5d332f-1aa2-47a6-ba39-0694c2f3112a"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<html><body><p> Movie 1</p><p> Actor - Aamir Khan</p><p> Click here to <a href='\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dictionary of common chat words and their full expansions.\n",
        "# This dictionary is used to convert informal chat abbreviations into their formal forms\n",
        "# to improve text readability and consistency for natural language processing tasks.\n",
        "chat_words = {\n",
        "    'AFAIK':'As Far As I Know',\n",
        "    'AFK':'Away From Keyboard',\n",
        "    'ASAP':'As Soon As Possible',\n",
        "    \"FYI\": \"For Your Information\",\n",
        "    \"ASAP\": \"As Soon As Possible\",\n",
        "    \"BRB\": \"Be Right Back\",\n",
        "    \"BTW\": \"By The Way\",\n",
        "    \"OMG\": \"Oh My God\",\n",
        "    \"IMO\": \"In My Opinion\",\n",
        "    \"LOL\": \"Laugh Out Loud\",\n",
        "    \"TTYL\": \"Talk To You Later\",\n",
        "    \"GTG\": \"Got To Go\",\n",
        "    \"TTYT\": \"Talk To You Tomorrow\",\n",
        "    \"IDK\": \"I Don't Know\",\n",
        "    \"TMI\": \"Too Much Information\",\n",
        "    \"IMHO\": \"In My Humble Opinion\",\n",
        "    \"ICYMI\": \"In Case You Missed It\",\n",
        "    \"AFAIK\": \"As Far As I Know\",\n",
        "    \"BTW\": \"By The Way\",\n",
        "    \"FAQ\": \"Frequently Asked Questions\",\n",
        "    \"TGIF\": \"Thank God It's Friday\",\n",
        "    \"FYA\": \"For Your Action\",\n",
        "    \"ICYMI\": \"In Case You Missed It\",\n",
        "}"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6W4GXnQEyl2q"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function called chat_conversion that takes a text string as input.\n",
        "# This function iterates through the words in the input text and replaces\n",
        "# any chat abbreviations found in the chat_words dictionary with their full forms.\n",
        "def chat_conversion(text):\n",
        "    new_text = [] # Initialize an empty list to store the converted words.\n",
        "    # Split the input text into individual words using spaces as delimiters\n",
        "    # and iterate through each word.\n",
        "    for w in text.split():\n",
        "        # Convert the current word to uppercase and check if it exists as a key\n",
        "        # in the chat_words dictionary.\n",
        "        if w.upper() in chat_words:\n",
        "            # If the uppercase word is found in the dictionary, append its corresponding\n",
        "            # full form (value) to the new_text list.\n",
        "            new_text.append(chat_words[w.upper()])\n",
        "        else:\n",
        "            # If the word is not found in the dictionary, append the original word\n",
        "            # to the new_text list without any changes.\n",
        "            new_text.append(w)\n",
        "    # Join all the words in the new_text list back into a single string,\n",
        "    # separated by spaces, and return the resulting string.\n",
        "    return \" \".join(new_text)\n",
        "\n",
        "# Demonstrate the usage of the chat_conversion function with example inputs\n",
        "# and print the converted text.\n",
        "print(chat_conversion(\"ASAP\"))\n",
        "print(chat_conversion(\"FAQ\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYM_LvMIzGtL",
        "outputId": "422b0420-aa04-420f-a8ed-6928268319ab"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As Soon As Possible\n",
            "Frequently Asked Questions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrate the usage of the chat_conversion function with a sentence\n",
        "# that includes a chat word (\"ASAP\") and print the converted sentence.\n",
        "print(chat_conversion('Do this work ASAP'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOvO3HQvzNhr",
        "outputId": "d08a62ef-249d-4ba5-fa6a-68e13823091f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Do this work As Soon As Possible\n"
          ]
        }
      ]
    }
  ]
}