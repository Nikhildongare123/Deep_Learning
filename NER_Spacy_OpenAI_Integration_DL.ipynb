{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#NER stands for Named Entity Recognition.\n",
        "\n",
        "It is a Natural Language Processing (NLP) technique used to automatically identify and classify named entities in text into predefined categories such as:\n",
        "\n",
        "1) Person names (e.g., Albert Einstein)\n",
        "2. Organizations (e.g., Google)\n",
        "   \n",
        "3. Locations (e.g., Paris)\n",
        "\n",
        "4.  Dates (e.g., 6th October 2025)\n",
        "\n",
        "5.  Miscellaneous entities (e.g., currency, percentages, product names)\n",
        "\n",
        "##Example:\n",
        "Input: \"Apple was founded by Steve Jobs in California in 1976.\"\n",
        "\n",
        "###NER Output:\n",
        "\n",
        "    Apple → Organization\n",
        "    Steve Jobs → Person\n",
        "    California → Location\n",
        "    1976 → Date\n",
        "\n",
        "In short, NER helps machines “understand” specific entities in text, which is crucial for tasks like information extraction, question answering, and chatbots.\n",
        "\n",
        "#NER Spacy OpenAi-Integration\n",
        "\n",
        "It is the combination of SpaCy’s fast Named Entity Recognition (NER) with OpenAI’s GPT models to detect, classify, and extract entities from text, allowing both structured (SpaCy) and flexible/custom/context-aware (OpenAI) entity recognition in a single workflow.\n",
        "\n",
        "In short: SpaCy for speed and structure, OpenAI for context and customization."
      ],
      "metadata": {
        "id": "9y-yH6tZ6m8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai # Import the openai library for interacting with the OpenAI API.\n",
        "import spacy # Import the spacy library for natural language processing tasks, specifically Named Entity Recognition (NER).\n",
        "!pip install openai==0.28 # Install a specific version of the openai library (version 0.28)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62yhOWT_dUSU",
        "outputId": "014757ae-157c-4099-b05c-39c1525f3b32"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.12/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.12/dist-packages (from openai==0.28) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from openai==0.28) (3.12.15)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->openai==0.28) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->openai==0.28) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->openai==0.28) (2025.8.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->openai==0.28) (4.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Spacy NER model\n",
        "nlp = spacy.load(\"en_core_web_sm\") # Load the English small model for spaCy, which includes a NER component.\n",
        "\n",
        "\n",
        "# Predefined list of stock market-related entities (extendable)\n",
        "stock_entities = [\"Apple\", \"Google\", \"Microsoft\", \"Tesla\", \"NASDAQ\", \"Dow Jones\", \"S&P 500\", \"Bitcoin\", \"Ethereum\"] # Define a list of entities related to the stock market to be recognized."
      ],
      "metadata": {
        "id": "rPGkVKEXeEJw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sentiment(review, category):\n",
        "    # Construct a prompt for the OpenAI API to perform sentiment analysis.\n",
        "    prompt = f\"Analyze the sentiment of the following {category} statement in the context of stock market performance. \\\n",
        "    Classify it as Positive (bullish), Negative (bearish), or Neutral:\\n\\nStatement: {review}\"\n",
        "\n",
        "    # Make a call to the OpenAI Chat Completion API to get the sentiment.\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\", # Specify the model to use for the API call.\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a sentiment analysis assistant specialized in financial markets.\"}, # Define the role and behavior of the AI assistant.\n",
        "            {\"role\": \"user\", \"content\": prompt}, # Provide the user's input (the prompt).\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Extract the sentiment classification from the API response.\n",
        "    sentiment = response['choices'][0]['message']['content']\n",
        "    return sentiment.strip() # Return the extracted sentiment, removing leading/trailing whitespace."
      ],
      "metadata": {
        "id": "-Gf4HeX2erqY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Function to perform Named Entity Recognition (NER)\n",
        "def extract_entities(review):\n",
        "    doc = nlp(review) # Process the input text using the loaded spaCy model to create a Doc object.\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents] # Extract entities and their labels from the Doc object.\n",
        "\n",
        "    # Additional filtering for financial entities\n",
        "    matched_stocks = [ent for ent in entities if ent[0] in stock_entities] # Filter the extracted entities to find those present in the predefined stock_entities list.\n",
        "    return matched_stocks if matched_stocks else entities  # Return the matched stock entities if any are found, otherwise return all extracted entities."
      ],
      "metadata": {
        "id": "myqmlscafXF0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function to perform sentiment analysis and NER\n",
        "def main():\n",
        "    # Get input from user\n",
        "    category = input(\"Enter the category (e.g., Stock, Index, Crypto, Economy, Other): \").capitalize() # Prompt the user to enter a category and capitalize the input.\n",
        "    review = input(f\"Enter your market statement related to {category.lower()}: \") # Prompt the user to enter a market statement related to the chosen category.\n",
        "\n",
        "    if review: # Check if the user entered a statement.\n",
        "        print(\"\\nPerforming Stock Market Sentiment Analysis...\\n\") # Inform the user that sentiment analysis is being performed.\n",
        "        sentiment_with_contributions = analyze_sentiment(review, category) # Call the analyze_sentiment function to get the sentiment.\n",
        "        print(f\"Sentiment Analysis Result: {sentiment_with_contributions}\") # Print the sentiment analysis result.\n",
        "\n",
        "        print(\"\\nPerforming Named Entity Recognition (NER)...\\n\") # Inform the user that NER is being performed.\n",
        "        entities = extract_entities(review) # Call the extract_entities function to get the entities.\n",
        "        print(f\"Identified Market Entities: {entities}\") # Print the identified entities.\n",
        "    else:\n",
        "        print(\"Please enter a valid statement.\") # Prompt the user to enter a valid statement if the input was empty.\n",
        "\n",
        "# Run the main function\n",
        "main() # Execute the main function."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6Xp_35ufqPD",
        "outputId": "c5352faf-2cc4-49c9-8655-5a5fc81990a4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the category (e.g., Stock, Index, Crypto, Economy, Other): Crypto\n",
            "Enter your market statement related to crypto: Bitcoin dipped slightly to $123,745 after hitting an all-time high, yet maintained strong weekly gains. Robust ETF inflows and institutional demand fuel this rally, with experts anticipating further upward movement. Despite minor corrections, the overall crypto market remains bullish, driven by rising confidence.\n",
            "\n",
            "Performing Stock Market Sentiment Analysis...\n",
            "\n",
            "Sentiment Analysis Result: This statement can be classified as Positive (bullish) in the context of stock market performance. The mention of Bitcoin maintaining strong weekly gains, robust ETF inflows, institutional demand, experts anticipating further upward movement, and rising confidence in the overall crypto market all point to a positive outlook for the market.\n",
            "\n",
            "Performing Named Entity Recognition (NER)...\n",
            "\n",
            "Identified Market Entities: [('123,745', 'MONEY'), ('weekly', 'DATE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################################################################################################################################################################"
      ],
      "metadata": {
        "id": "6vB2Qh_Yhb63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56283156",
        "outputId": "ce0051c2-f37e-4a94-ae6f-073061d93ac1"
      },
      "source": [
        "!pip install PyPDF2 # Install the PyPDF2 library for working with PDF files."
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/232.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 🔧 Install Dependencies\n",
        "# ====================================================\n",
        "!pip install -q openai PyPDF2 # Install the openai and PyPDF2 libraries silently.\n",
        "\n",
        "# ====================================================\n",
        "# 📚 Import Libraries\n",
        "# ====================================================\n",
        "import openai # Import the openai library.\n",
        "import PyPDF2 # Import the PyPDF2 library.\n",
        "from google.colab import files # Import the files module from google.colab for file uploads.\n",
        "from google.colab import userdata # Import userdata to access secrets\n",
        "\n",
        "# ====================================================\n",
        "# 🔑 Set your OpenAI API key from Secrets Manager\n",
        "# ====================================================\n",
        "# Access the API key stored in Colab's Secrets Manager\n",
        "# Make sure you have added a secret named 'OPENAI_API_KEY'\n",
        "try:\n",
        "    openai.api_key = userdata.get('OPENAI_API_KEY') # Attempt to retrieve the OpenAI API key from Colab's Secrets Manager.\n",
        "    if not openai.api_key: # Check if the API key was successfully retrieved.\n",
        "        raise ValueError(\"OpenAI API key not found in Colab Secrets. Please add it using the 🔑 icon.\") # Raise a ValueError if the API key is not found.\n",
        "except Exception as e: # Catch any exceptions that occur during the process.\n",
        "    print(f\"Error accessing Secrets Manager: {e}\") # Print an error message.\n",
        "    # You might want to handle this error more gracefully,\n",
        "    # e.g., prompt the user to add the key.\n",
        "    raise # Re-raise the exception after printing the message\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# 📂 Function to upload PDF file\n",
        "# ====================================================\n",
        "def upload_pdf():\n",
        "    print(\"📁 Please upload your research PDF file:\") # Prompt the user to upload a PDF file.\n",
        "    uploaded = files.upload() # Use google.colab.files.upload() to handle the file upload.\n",
        "    file_name = next(iter(uploaded)) # Get the name of the uploaded file.\n",
        "    print(f\"✅ Uploaded: {file_name}\") # Confirm the file upload with the file name.\n",
        "    return file_name # Return the name of the uploaded file.\n",
        "\n",
        "# ====================================================\n",
        "# 📄 Function to extract text from PDF\n",
        "# ====================================================\n",
        "def extract_text_from_pdf(file_path):\n",
        "    text = \"\" # Initialize an empty string to store the extracted text.\n",
        "    with open(file_path, \"rb\") as pdf_file: # Open the PDF file in binary read mode.\n",
        "        reader = PyPDF2.PdfReader(pdf_file) # Create a PdfReader object.\n",
        "        for page in reader.pages: # Iterate through each page in the PDF.\n",
        "            text += page.extract_text() or \"\" # Extract text from the current page and append it to the text string. Use or \"\" to handle pages with no text.\n",
        "    return text.strip() # Return the extracted text, removing leading/trailing whitespace.\n",
        "\n",
        "# ====================================================\n",
        "# 🤖 Function to extract author name using OpenAI\n",
        "# ====================================================\n",
        "def extract_author_from_text(research_text):\n",
        "    # Define the prompt for the OpenAI API to extract the author name.\n",
        "    prompt = f\"\"\"\n",
        "    You are a research paper analyzer.\n",
        "    Extract only the author name(s) from the text below.\n",
        "    If not found, respond with \"Author not found\".\n",
        "\n",
        "    Research text:\n",
        "    \\\"\\\"\\\"{research_text[:5000]}\\\"\\\"\\\"\n",
        "    \"\"\"\n",
        "    # Make a call to the OpenAI Chat Completion API to get the author name.\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\", # Specify the model to use.\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}], # Provide the user's input (the prompt).\n",
        "        temperature=0 # Set the temperature to 0 for deterministic output.\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"].strip() # Extract and return the author name from the API response.\n",
        "\n",
        "# ====================================================\n",
        "# 📘 Extract Title and Abstract\n",
        "# ====================================================\n",
        "def extract_title_abstract_from_text(research_text):\n",
        "    # Define the prompt for the OpenAI API to extract the title and abstract.\n",
        "    prompt = f\"\"\"\n",
        "    You are a research paper analyzer.\n",
        "    From the following research text, extract:\n",
        "    - The title of the paper\n",
        "    - The abstract of the paper\n",
        "\n",
        "    Return the output in a structured JSON format with keys: title, abstract.\n",
        "    If a title or abstract is not found, use \"Not found\" for that key.\n",
        "\n",
        "    Research text:\n",
        "    \\\"\\\"\\\"{research_text[:5000]}\\\"\\\"\\\"\n",
        "    \"\"\"\n",
        "    # Make a call to the OpenAI Chat Completion API to get the title and abstract.\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\", # Specify the model to use.\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}], # Provide the user's input (the prompt).\n",
        "        temperature=0 # Set the temperature to 0 for deterministic output.\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"].strip() # Extract and return the title and abstract in JSON format from the API response.\n",
        "\n",
        "# ====================================================\n",
        "# 🚀 Main Function\n",
        "# ====================================================\n",
        "def main():\n",
        "    file_path = upload_pdf() # Call the upload_pdf function to get the file path of the uploaded PDF.\n",
        "    print(\"\\n📖 Extracting text from PDF...\\n\") # Inform the user that text extraction is in progress.\n",
        "    research_text = extract_text_from_pdf(file_path) # Call the extract_text_from_pdf function to get the text from the PDF.\n",
        "\n",
        "    print(\"🔍 Extracting author details using OpenAI...\\n\") # Inform the user that author extraction is in progress.\n",
        "    author_info = extract_author_from_text(research_text) # Call the extract_author_from_text function to get the author information.\n",
        "    print(f\"🧾 Extracted Author(s): {author_info}\") # Print the extracted author information.\n",
        "\n",
        "    print(\"\\n🔍 Extracting title and abstract using OpenAI...\\n\") # Inform the user that title and abstract extraction is in progress.\n",
        "    title_abstract_info = extract_title_abstract_from_text(research_text) # Call the extract_title_abstract_from_text function to get the title and abstract.\n",
        "    print(f\"📄 Extracted Title and Abstract:\\n{title_abstract_info}\") # Print the extracted title and abstract.\n",
        "\n",
        "# ====================================================\n",
        "# ▶️ Run\n",
        "# ====================================================\n",
        "main() # Execute the main function to start the process."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "ad4wBjmwlgZb",
        "outputId": "c186bef1-0374-42ab-b587-a6c59fd6cb44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📁 Please upload your research PDF file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-308344e4-0fdc-4e14-aea8-ac6252474a65\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-308344e4-0fdc-4e14-aea8-ac6252474a65\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Research paper.pdf to Research paper (9).pdf\n",
            "✅ Uploaded: Research paper (9).pdf\n",
            "\n",
            "📖 Extracting text from PDF...\n",
            "\n",
            "🔍 Extracting author details using OpenAI...\n",
            "\n",
            "🧾 Extracted Author(s): Author: Mariusz Kruk\n",
            "\n",
            "🔍 Extracting title and abstract using OpenAI...\n",
            "\n",
            "📄 Extracted Title and Abstract:\n",
            "{\n",
            "    \"title\": \"A look at advanced learners’ use of mobile devices for English language study: Insights from interview data\",\n",
            "    \"abstract\": \"The paper discusses the results of a study which explored advanced learners of English engagement with their mobile devices to develop learning experiences that meet their needs and goals as foreign language learners. The data were collected from 20 students by means of a semi-structured interview. The gathered data were subjected to qualitative and quantitative analysis. The results of the study demonstrated that, on the one hand, some subjects manifested heightened awareness relating to the advantageous role of mobile devices in their learning endeavors, their ability to reach for suitable tools and retrieve necessary information so as to achieve their goals, meet their needs and adjust their learning of English to their personal learning styles, and on the other, a rather intuitive and/or ad hoc use of their mobile devices in the classroom. Keywords: Learner autonomy, mobile devices, advanced EFL learners, learning English.\"\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}